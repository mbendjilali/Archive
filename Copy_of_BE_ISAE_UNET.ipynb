{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "Copy of BE_ISAE_UNET.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "\n",
        "### In this notebook we use a [UNet](https://arxiv.org/abs/1505.04597) segmentation model for performing building segmentation on Massachusetts Buildings Dataset.\n",
        "\n",
        "Start by changing runtime to GPU\n",
        "\n",
        "Then, we will upgrade some packages"
      ],
      "metadata": {
        "id": "oy7WtrBzZg1A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U albumentations\n",
        "!pip uninstall opencv\n",
        "!pip install --upgrade opencv-python\n",
        "!pip install segmentation-models-pytorch"
      ],
      "metadata": {
        "id": "JtAb-1bQ3cSa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Libraries ðŸ“šâ¬‡"
      ],
      "metadata": {
        "id": "ps9Vs9epZg1F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import random, tqdm\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "import albumentations as album\n",
        "import segmentation_models_pytorch as smp"
      ],
      "metadata": {
        "trusted": true,
        "id": "PaVNFJmeZg1j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data\n",
        "- Download the dataset at [this address](https://drive.google.com/u/0/uc?id=1NmtroJolER0N17vj_apIyer6bKb2aNhX&export=download)\n",
        "- Copy to your Google Drive\n",
        "- Decompress and define train/val/test directories"
      ],
      "metadata": {
        "id": "yppuzoShbaD4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "GfUMSZYIpz1v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir drive/MyDrive/BE_UNET_data/\n",
        "!unzip drive/MyDrive/archive.zip -d drive/MyDrive/BE_UNET_data/"
      ],
      "metadata": {
        "id": "WCtrdqm2c0A3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_DIR = 'drive/MyDrive/BE_UNET_data/'\n",
        "\n",
        "x_train_dir = os.path.join(DATA_DIR + 'tiff/', 'train')\n",
        "y_train_dir = os.path.join(DATA_DIR + 'tiff/', 'train_labels')\n",
        "\n",
        "x_valid_dir = os.path.join(DATA_DIR + 'tiff/', 'val')\n",
        "y_valid_dir = os.path.join(DATA_DIR + 'tiff/', 'val_labels')\n",
        "\n",
        "x_test_dir = os.path.join(DATA_DIR + 'tiff/', 'test')\n",
        "y_test_dir = os.path.join(DATA_DIR + 'tiff/', 'test_labels')"
      ],
      "metadata": {
        "trusted": true,
        "id": "_ELum9RZZg1q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_dict = pd.read_csv(DATA_DIR + \"label_class_dict.csv\")\n",
        "# Get class names\n",
        "class_names = class_dict['name'].tolist()\n",
        "# Get class RGB values\n",
        "class_rgb_values = class_dict[['r','g','b']].values.tolist()\n",
        "\n",
        "print('All dataset classes and their corresponding RGB values in labels:')\n",
        "print('Class Names: ', class_names)\n",
        "print('Class RGB values: ', class_rgb_values)"
      ],
      "metadata": {
        "trusted": true,
        "id": "cRyVJy5gZg1s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Shortlist specific classes to segment"
      ],
      "metadata": {
        "id": "l0HdPMB5Zg1u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Useful to shortlist specific classes in datasets with large number of classes\n",
        "select_classes = ['background', 'building']\n",
        "\n",
        "# Get RGB values of required classes\n",
        "select_class_indices = [class_names.index(cls.lower()) for cls in select_classes]\n",
        "select_class_rgb_values =  np.array(class_rgb_values)[select_class_indices]\n",
        "\n",
        "print('Selected classes and their corresponding RGB values in labels:')\n",
        "print('Class Names: ', class_names)\n",
        "print('Class RGB values: ', class_rgb_values)"
      ],
      "metadata": {
        "trusted": true,
        "id": "2GkmBF0TZg1v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Helper functions for viz. & one-hot encoding/decoding"
      ],
      "metadata": {
        "id": "NVMHbhjCZg1y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# helper function for data visualization\n",
        "def visualize(**images):\n",
        "    \"\"\"\n",
        "    Plot images in one row\n",
        "    \"\"\"\n",
        "    n_images = len(images)\n",
        "    plt.figure(figsize=(20,8))\n",
        "    for idx, (name, image) in enumerate(images.items()):\n",
        "        plt.subplot(1, n_images, idx + 1)\n",
        "        plt.xticks([]); \n",
        "        plt.yticks([])\n",
        "        # get title from the parameter names\n",
        "        plt.title(name.replace('_',' ').title(), fontsize=20)\n",
        "        plt.imshow(image)\n",
        "    plt.show()\n",
        "\n",
        "# Perform one hot encoding on label\n",
        "def one_hot_encode(label, label_values):\n",
        "    \"\"\"\n",
        "    Convert a segmentation image label array to one-hot format\n",
        "    by replacing each pixel value with a vector of length num_classes\n",
        "    # Arguments\n",
        "        label: The 2D array segmentation image label\n",
        "        label_values\n",
        "        \n",
        "    # Returns\n",
        "        A 2D array with the same width and hieght as the input, but\n",
        "        with a depth size of num_classes\n",
        "    \"\"\"\n",
        "    semantic_map = []\n",
        "    for colour in label_values:\n",
        "        equality = np.equal(label, colour)\n",
        "        class_map = np.all(equality, axis = -1)\n",
        "        semantic_map.append(class_map)\n",
        "    semantic_map = np.stack(semantic_map, axis=-1)\n",
        "\n",
        "    return semantic_map\n",
        "    \n",
        "# Perform reverse one-hot-encoding on labels / preds\n",
        "def reverse_one_hot(image):\n",
        "    \"\"\"\n",
        "    Transform a 2D array in one-hot format (depth is num_classes),\n",
        "    to a 2D array with only 1 channel, where each pixel value is\n",
        "    the classified class key.\n",
        "    # Arguments\n",
        "        image: The one-hot format image \n",
        "        \n",
        "    # Returns\n",
        "        A 2D array with the same width and hieght as the input, but\n",
        "        with a depth size of 1, where each pixel value is the classified \n",
        "        class key.\n",
        "    \"\"\"\n",
        "    x = np.argmax(image, axis = -1)\n",
        "    return x\n",
        "\n",
        "# Perform colour coding on the reverse-one-hot outputs\n",
        "def colour_code_segmentation(image, label_values):\n",
        "    \"\"\"\n",
        "    Given a 1-channel array of class keys, colour code the segmentation results.\n",
        "    # Arguments\n",
        "        image: single channel array where each value represents the class key.\n",
        "        label_values\n",
        "\n",
        "    # Returns\n",
        "        Colour coded image for segmentation visualization\n",
        "    \"\"\"\n",
        "    colour_codes = np.array(label_values)\n",
        "    x = colour_codes[image.astype(int)]\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "trusted": true,
        "id": "e46dRyUbZg1z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BuildingsDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    \"\"\"Massachusetts Buildings Dataset. Read images, apply augmentation and preprocessing transformations.\n",
        "    \n",
        "    Args:\n",
        "        images_dir (str): path to images folder\n",
        "        masks_dir (str): path to segmentation masks folder\n",
        "        class_rgb_values (list): RGB values of select classes to extract from segmentation mask\n",
        "        augmentation (albumentations.Compose): data transfromation pipeline \n",
        "            (e.g. flip, scale, etc.)\n",
        "        preprocessing (albumentations.Compose): data preprocessing \n",
        "            (e.g. noralization, shape manipulation, etc.)\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "            self, \n",
        "            images_dir, \n",
        "            masks_dir, \n",
        "            class_rgb_values=None, \n",
        "            augmentation=None, \n",
        "            preprocessing=None,\n",
        "    ):\n",
        "        \n",
        "        self.image_paths = [os.path.join(images_dir, image_id) for image_id in sorted(os.listdir(images_dir))]\n",
        "        self.mask_paths = [os.path.join(masks_dir, image_id) for image_id in sorted(os.listdir(masks_dir))]\n",
        "\n",
        "        self.class_rgb_values = class_rgb_values\n",
        "        self.augmentation = augmentation\n",
        "        self.preprocessing = preprocessing\n",
        "    \n",
        "    def __getitem__(self, i):\n",
        "        \n",
        "        # read images and masks\n",
        "        image = cv2.cvtColor(cv2.imread(self.image_paths[i]), cv2.COLOR_BGR2RGB)\n",
        "        mask = cv2.cvtColor(cv2.imread(self.mask_paths[i]), cv2.COLOR_BGR2RGB)\n",
        "        \n",
        "        # one-hot-encode the mask\n",
        "        mask = one_hot_encode(mask, self.class_rgb_values).astype('float')\n",
        "        \n",
        "        # apply augmentations\n",
        "        if self.augmentation:\n",
        "            sample = self.augmentation(image=image, mask=mask)\n",
        "            image, mask = sample['image'], sample['mask']\n",
        "        \n",
        "        # apply preprocessing\n",
        "        if self.preprocessing:\n",
        "            sample = self.preprocessing(image=image, mask=mask)\n",
        "            image, mask = sample['image'], sample['mask']\n",
        "            \n",
        "        return image, mask\n",
        "        \n",
        "    def __len__(self):\n",
        "        # return length of \n",
        "        return len(self.image_paths)"
      ],
      "metadata": {
        "trusted": true,
        "id": "6-zKq6gfZg12"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Visualize Sample Image and Mask ðŸ“ˆ"
      ],
      "metadata": {
        "id": "NbXNugSnZg15"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = BuildingsDataset(x_train_dir, y_train_dir, class_rgb_values=select_class_rgb_values)\n",
        "random_idx = random.randint(0, len(dataset)-1)\n",
        "image, mask = dataset[2]\n",
        "\n",
        "visualize(\n",
        "    original_image = image,\n",
        "    ground_truth_mask = colour_code_segmentation(reverse_one_hot(mask), select_class_rgb_values),\n",
        "    one_hot_encoded_mask = reverse_one_hot(mask)\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "id": "d_P4dZgYZg16"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining Augmentations ðŸ™ƒ"
      ],
      "metadata": {
        "id": "HcV5JQM1Zg17"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_training_augmentation():\n",
        "    train_transform = [    \n",
        "        album.RandomCrop(height=256, width=256, always_apply=True),\n",
        "        album.OneOf(\n",
        "            [\n",
        "                album.HorizontalFlip(p=1),\n",
        "                album.VerticalFlip(p=1),\n",
        "                album.RandomRotate90(p=1),\n",
        "            ],\n",
        "            p=0.75,\n",
        "        ),\n",
        "    ]\n",
        "    return album.Compose(train_transform)\n",
        "\n",
        "\n",
        "def get_validation_augmentation():   \n",
        "    # Add sufficient padding to ensure image is divisible by 32\n",
        "    test_transform = [\n",
        "        album.PadIfNeeded(min_height=1536, min_width=1536, always_apply=True, border_mode=0),\n",
        "    ]\n",
        "    return album.Compose(test_transform)\n",
        "\n",
        "\n",
        "def to_tensor(x, **kwargs):\n",
        "    return x.transpose(2, 0, 1).astype('float32')\n",
        "\n",
        "\n",
        "def get_preprocessing(preprocessing_fn=None):\n",
        "    \"\"\"Construct preprocessing transform    \n",
        "    Args:\n",
        "        preprocessing_fn (callable): data normalization function \n",
        "            (can be specific for each pretrained neural network)\n",
        "    Return:\n",
        "        transform: albumentations.Compose\n",
        "    \"\"\"   \n",
        "    _transform = []\n",
        "    if preprocessing_fn:\n",
        "        _transform.append(album.Lambda(image=preprocessing_fn))\n",
        "    _transform.append(album.Lambda(image=to_tensor, mask=to_tensor))\n",
        "        \n",
        "    return album.Compose(_transform)"
      ],
      "metadata": {
        "trusted": true,
        "id": "KBqoN_IaZg18"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Visualize Augmented Images & Masks"
      ],
      "metadata": {
        "id": "mGxUzxZ0Zg1-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "augmented_dataset = BuildingsDataset(\n",
        "    x_train_dir, y_train_dir, \n",
        "    augmentation=get_training_augmentation(),\n",
        "    class_rgb_values=select_class_rgb_values,\n",
        ")\n",
        "\n",
        "random_idx = random.randint(0, len(augmented_dataset)-1)\n",
        "\n",
        "# Different augmentations on a random image/mask pair (256*256 crop)\n",
        "for i in range(3):\n",
        "    image, mask = augmented_dataset[random_idx]\n",
        "    visualize(\n",
        "        original_image = image,\n",
        "        ground_truth_mask = colour_code_segmentation(reverse_one_hot(mask), select_class_rgb_values),\n",
        "        one_hot_encoded_mask = reverse_one_hot(mask)\n",
        "    )"
      ],
      "metadata": {
        "trusted": true,
        "id": "cC3IJ-JMZg1_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training UNet"
      ],
      "metadata": {
        "id": "e6ZBHmxhZg2A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3><center>UNet Model Architecture</center></h3>\n",
        "<img src=\"https://miro.medium.com/max/2824/1*f7YOaE4TWubwaFF7Z1fzNw.png\" width=\"750\" height=\"750\"/>\n",
        "<h4><center><a href=\"https://arxiv.org/abs/1505.04597\">Image Courtesy: UNet [Ronneberger et al.]</a></center></h4>"
      ],
      "metadata": {
        "id": "9mwjFRa2Zg2B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Definition"
      ],
      "metadata": {
        "id": "WVVVooCBZg2C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DoubleConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(DoubleConv, self).__init__()\n",
        "        self.double_conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.double_conv(x)\n",
        "    \n",
        "    \n",
        "class DownBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(DownBlock, self).__init__()\n",
        "        self.double_conv = DoubleConv(in_channels, out_channels)\n",
        "        self.down_sample = nn.MaxPool2d(2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        skip_out = self.double_conv(x)\n",
        "        down_out = self.down_sample(skip_out)\n",
        "        return (down_out, skip_out)\n",
        "\n",
        "    \n",
        "class UpBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, up_sample_mode):\n",
        "        super(UpBlock, self).__init__()\n",
        "        if up_sample_mode == 'conv_transpose':\n",
        "            self.up_sample = nn.ConvTranspose2d(in_channels-out_channels, in_channels-out_channels, kernel_size=2, stride=2)        \n",
        "        elif up_sample_mode == 'bilinear':\n",
        "            self.up_sample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported `up_sample_mode` (can take one of `conv_transpose` or `bilinear`)\")\n",
        "        self.double_conv = DoubleConv(in_channels, out_channels)\n",
        "\n",
        "    def forward(self, down_input, skip_input):\n",
        "        x = self.up_sample(down_input)\n",
        "        x = torch.cat([x, skip_input], dim=1)\n",
        "        return self.double_conv(x)\n",
        "\n",
        "    \n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, out_classes=2, up_sample_mode='conv_transpose'):\n",
        "        super(UNet, self).__init__()\n",
        "        self.up_sample_mode = up_sample_mode\n",
        "        # Downsampling Path\n",
        "        self.down_conv1 = DownBlock(3, 64)\n",
        "        self.down_conv2 = DownBlock(64, 128)\n",
        "        self.down_conv3 = DownBlock(128, 256)\n",
        "        self.down_conv4 = DownBlock(256, 512)\n",
        "        # Bottleneck\n",
        "        self.double_conv = DoubleConv(512, 1024)\n",
        "        # Upsampling Path\n",
        "        self.up_conv4 = UpBlock(512 + 1024, 512, self.up_sample_mode)\n",
        "        self.up_conv3 = UpBlock(256 + 512, 256, self.up_sample_mode)\n",
        "        self.up_conv2 = UpBlock(128 + 256, 128, self.up_sample_mode)\n",
        "        self.up_conv1 = UpBlock(128 + 64, 64, self.up_sample_mode)\n",
        "        # Final Convolution\n",
        "        self.conv_last = nn.Conv2d(64, out_classes, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x, skip1_out = self.down_conv1(x)\n",
        "        x, skip2_out = self.down_conv2(x)\n",
        "        x, skip3_out = self.down_conv3(x)\n",
        "        x, skip4_out = self.down_conv4(x)\n",
        "        x = self.double_conv(x)\n",
        "        x = self.up_conv4(x, skip4_out)\n",
        "        x = self.up_conv3(x, skip3_out)\n",
        "        x = self.up_conv2(x, skip2_out)\n",
        "        x = self.up_conv1(x, skip1_out)\n",
        "        x = self.conv_last(x)\n",
        "        return x\n",
        "    \n",
        "\n",
        "# Get UNet model\n",
        "model = UNet()"
      ],
      "metadata": {
        "trusted": true,
        "id": "ceEfHQdGZg2D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Get Train / Val DataLoaders"
      ],
      "metadata": {
        "id": "fPml-3V6Zg2F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get train and val dataset instances\n",
        "train_dataset = BuildingsDataset(\n",
        "    x_train_dir, y_train_dir, \n",
        "    augmentation=get_training_augmentation(),\n",
        "    preprocessing=get_preprocessing(preprocessing_fn=None),\n",
        "    class_rgb_values=select_class_rgb_values,\n",
        ")\n",
        "\n",
        "valid_dataset = BuildingsDataset(\n",
        "    x_valid_dir, y_valid_dir, \n",
        "    augmentation=get_validation_augmentation(), \n",
        "    preprocessing=get_preprocessing(preprocessing_fn=None),\n",
        "    class_rgb_values=select_class_rgb_values,\n",
        ")\n",
        "\n",
        "# Get train and val data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=12)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=1, shuffle=False, num_workers=4)"
      ],
      "metadata": {
        "trusted": true,
        "id": "2PiuXimMZg2G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Set Hyperparams"
      ],
      "metadata": {
        "id": "ozByZOpXZg2H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set flag to train the model or not. If set to 'False', only prediction is performed (using an older model checkpoint)\n",
        "TRAINING = True\n",
        "\n",
        "# Set num of epochs\n",
        "EPOCHS = 12\n",
        "\n",
        "# Set device: `cuda` or `cpu`\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(DEVICE)\n",
        "\n",
        "# define loss function\n",
        "loss = smp.utils.losses.DiceLoss()\n",
        "\n",
        "# define metrics\n",
        "metrics = [\n",
        "    smp.utils.metrics.IoU(threshold=0.5),\n",
        "]\n",
        "\n",
        "# define optimizer\n",
        "optimizer = torch.optim.Adam([ \n",
        "    dict(params=model.parameters(), lr=0.00008),\n",
        "])\n",
        "\n",
        "# define learning rate scheduler (not used in this NB)\n",
        "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
        "    optimizer, T_0=1, T_mult=2, eta_min=5e-5,\n",
        ")\n",
        "\n",
        "# load best saved model checkpoint from previous commit (if present)\n",
        "# if os.path.exists(DATA_DIR + 'best_model.pth'):\n",
        "#    model = torch.load(DATA_DIR + 'best_model.pth', map_location=DEVICE)"
      ],
      "metadata": {
        "trusted": true,
        "id": "ioTG9BPDZg2I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_epoch = smp.utils.train.TrainEpoch(\n",
        "    model, \n",
        "    loss=loss, \n",
        "    metrics=metrics, \n",
        "    optimizer=optimizer,\n",
        "    device=DEVICE,\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "valid_epoch = smp.utils.train.ValidEpoch(\n",
        "    model, \n",
        "    loss=loss, \n",
        "    metrics=metrics, \n",
        "    device=DEVICE,\n",
        "    verbose=True,\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "id": "mjqbNpjFZg2I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training UNet"
      ],
      "metadata": {
        "id": "S2BDC9xQZg2J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "if TRAINING:\n",
        "\n",
        "    best_iou_score = 0.0\n",
        "    train_logs_list, valid_logs_list = [], []\n",
        "\n",
        "    for i in range(0, EPOCHS):\n",
        "\n",
        "        # Perform training & validation\n",
        "        print('\\nEpoch: {}'.format(i))\n",
        "        train_logs = train_epoch.run(train_loader)\n",
        "        valid_logs = valid_epoch.run(valid_loader)\n",
        "        train_logs_list.append(train_logs)\n",
        "        valid_logs_list.append(valid_logs)\n",
        "\n",
        "        # Save model if a better val IoU score is obtained\n",
        "        if best_iou_score < valid_logs['iou_score']:\n",
        "            best_iou_score = valid_logs['iou_score']\n",
        "            torch.save(model, DATA_DIR + 'best_model.pth')\n",
        "            print('Model saved!')"
      ],
      "metadata": {
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "trusted": true,
        "id": "YRaASTrPZg2K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Questions sur le code\n",
        "\n",
        "L'entraÃ®nement prend environ 20 minutes sur les GPUs de Colab. Pendant ce temps, revenir sur le code pour rÃ©pondre aux questions suivantes :\n",
        "- Quelles sont les transformations utilisÃ©es pour l'augmentation de donnÃ©es?\n",
        "- Pourquoi a-t-on besoin de la fonction 'to_tensor'?\n",
        "- Comment est dÃ©finie la DICE loss?\n",
        "- Comment est dÃ©fini le critÃ¨re de qualitÃ© de la segmentation (IoU)?\n",
        "\n",
        "Revenir ensuite Ã  l'architecture UNET :\n",
        "- Comment sont dÃ©finies les couches de upsampling?\n",
        "- Quelle est la dimension spatiale des donnÃ©es Ã  travers le U-NET?\n",
        "- Comment sont dÃ©finies les *skip connections*?"
      ],
      "metadata": {
        "id": "g4S-CXe65aZF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Display losses"
      ],
      "metadata": {
        "id": "k1jYY-_Nf1fn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_logs_df = pd.DataFrame(train_logs_list)\n",
        "valid_logs_df = pd.DataFrame(valid_logs_list)\n",
        "train_logs_df.T\n",
        "\n",
        "# Dice loss\n",
        "plt.figure(figsize=(20,8))\n",
        "plt.plot(train_logs_df.index.tolist(), train_logs_df.dice_loss.tolist(), lw=3, label = 'Train')\n",
        "plt.plot(valid_logs_df.index.tolist(), valid_logs_df.dice_loss.tolist(), lw=3, label = 'Valid')\n",
        "plt.xlabel('Epochs', fontsize=21)\n",
        "plt.ylabel('Dice Loss', fontsize=21)\n",
        "plt.title('Dice Loss Plot', fontsize=21)\n",
        "plt.legend(loc='best', fontsize=16)\n",
        "plt.grid()\n",
        "plt.savefig('dice_loss_plot.png')\n",
        "plt.show()\n",
        "\n",
        "#IoU score\n",
        "plt.figure(figsize=(20,8))\n",
        "plt.plot(train_logs_df.index.tolist(), train_logs_df.iou_score.tolist(), lw=3, label = 'Train')\n",
        "plt.plot(valid_logs_df.index.tolist(), valid_logs_df.iou_score.tolist(), lw=3, label = 'Valid')\n",
        "plt.xlabel('Epochs', fontsize=21)\n",
        "plt.ylabel('IoU Score', fontsize=21)\n",
        "plt.title('IoU Score Plot', fontsize=21)\n",
        "plt.legend(loc='best', fontsize=16)\n",
        "plt.grid()\n",
        "plt.savefig('iou_score_plot.png')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4ddcjlcRgF6o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prediction on Test Data"
      ],
      "metadata": {
        "id": "SdPvC9JqZg2K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load best saved model checkpoint from the current run\n",
        "if os.path.exists(DATA_DIR + 'best_model.pth'):\n",
        "    best_model = torch.load(DATA_DIR + 'best_model.pth', map_location=DEVICE)\n",
        "    print('Loaded UNet model from this run.')\n",
        "\n",
        "# load best saved model checkpoint from previous commit (if present)\n",
        "elif os.path.exists('../input/unet-for-building-segmentation-pytorch/best_model.pth'):\n",
        "    best_model = torch.load('../input/unet-for-building-segmentation-pytorch/best_model.pth', map_location=DEVICE)\n",
        "    print('Loaded UNet model from a previous commit.')"
      ],
      "metadata": {
        "trusted": true,
        "id": "r6JvcjcOZg2K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create test dataloader to be used with UNet model (with preprocessing operation: to_tensor(...))\n",
        "test_dataset = BuildingsDataset(\n",
        "    x_test_dir, \n",
        "    y_test_dir, \n",
        "    augmentation=get_validation_augmentation(), \n",
        "    preprocessing=get_preprocessing(preprocessing_fn=None),\n",
        "    class_rgb_values=select_class_rgb_values,\n",
        ")\n",
        "\n",
        "test_dataloader = DataLoader(test_dataset)\n",
        "\n",
        "# test dataset for visualization (without preprocessing transformations)\n",
        "test_dataset_vis = BuildingsDataset(\n",
        "    x_test_dir, y_test_dir, \n",
        "    augmentation=get_validation_augmentation(),\n",
        "    class_rgb_values=select_class_rgb_values,\n",
        ")\n",
        "\n",
        "# get a random test image/mask index\n",
        "random_idx = random.randint(0, len(test_dataset_vis)-1)\n",
        "image, mask = test_dataset_vis[random_idx]\n",
        "\n",
        "visualize(\n",
        "    original_image = image,\n",
        "    ground_truth_mask = colour_code_segmentation(reverse_one_hot(mask), select_class_rgb_values),\n",
        "    one_hot_encoded_mask = reverse_one_hot(mask)\n",
        ")\n",
        "\n",
        "# Notice the images / masks are 1536*1536 because of 18px padding on all sides. \n",
        "# This is to ensure the input image dimensions to UNet model are a multiple of 2 (to account for pooling & transpose conv. operations)."
      ],
      "metadata": {
        "trusted": true,
        "id": "Byg3UXGVZg2L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Center crop padded image / mask to original image dims\n",
        "def crop_image(image, target_image_dims=[1500,1500,3]):\n",
        "   \n",
        "    target_size = target_image_dims[0]\n",
        "    image_size = len(image)\n",
        "    padding = (image_size - target_size) // 2\n",
        "\n",
        "    return image[\n",
        "        padding:image_size - padding,\n",
        "        padding:image_size - padding,\n",
        "        :,\n",
        "    ]"
      ],
      "metadata": {
        "trusted": true,
        "id": "1xNkP61nZg2L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_preds_folder = 'sample_predictions/'\n",
        "if not os.path.exists(sample_preds_folder):\n",
        "    os.makedirs(sample_preds_folder)"
      ],
      "metadata": {
        "trusted": true,
        "id": "ouSIfjkYZg2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for idx in range(len(test_dataset)):\n",
        "\n",
        "    random_idx = random.randint(0, len(test_dataset)-1)\n",
        "    image, gt_mask = test_dataset[random_idx]\n",
        "    image_vis = crop_image(test_dataset_vis[random_idx][0].astype('uint8'))\n",
        "    x_tensor = torch.from_numpy(image).to(DEVICE).unsqueeze(0)\n",
        "    # Predict test image\n",
        "    pred_mask = best_model(x_tensor)\n",
        "    pred_mask = pred_mask.detach().squeeze().cpu().numpy()\n",
        "    # Convert pred_mask from `CHW` format to `HWC` format\n",
        "    pred_mask = np.transpose(pred_mask,(1,2,0))\n",
        "    # Get prediction channel corresponding to building\n",
        "    pred_building_heatmap = pred_mask[:,:,select_classes.index('building')]\n",
        "    pred_mask = crop_image(colour_code_segmentation(reverse_one_hot(pred_mask), select_class_rgb_values))\n",
        "    # Convert gt_mask from `CHW` format to `HWC` format\n",
        "    gt_mask = np.transpose(gt_mask,(1,2,0))\n",
        "    gt_mask = crop_image(colour_code_segmentation(reverse_one_hot(gt_mask), select_class_rgb_values))\n",
        "    cv2.imwrite(os.path.join(sample_preds_folder, f\"sample_pred_{idx}.png\"), np.hstack([image_vis, gt_mask, pred_mask])[:,:,::-1])\n",
        "    \n",
        "    visualize(\n",
        "        original_image = image_vis,\n",
        "        ground_truth_mask = gt_mask,\n",
        "        predicted_mask = pred_mask,\n",
        "        predicted_building_heatmap = pred_building_heatmap\n",
        "    )"
      ],
      "metadata": {
        "trusted": true,
        "id": "imVrMF9jZg2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Evaluation on Test Dataset"
      ],
      "metadata": {
        "id": "WK8Lr4JlZg2N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_epoch = smp.utils.train.ValidEpoch(\n",
        "    model,\n",
        "    loss=loss, \n",
        "    metrics=metrics, \n",
        "    device=DEVICE,\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "valid_logs = test_epoch.run(test_dataloader)\n",
        "print(\"Evaluation on Test Data: \")\n",
        "print(f\"Mean IoU Score: {valid_logs['iou_score']:.4f}\")\n",
        "print(f\"Mean Dice Loss: {valid_logs['dice_loss']:.4f}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "Yw-IvfNgZg2N"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}